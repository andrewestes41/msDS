width_shift_range = 0.2,
height_shift_range = 0.2,
rotation_range = 20,
horizontal_flip = TRUE,
validation_split=0.2
)
test_image_flow <- flow_images_from_directory(
directory = "Test/",
generator = test_image_gen,
subset = "test",
class_mode = "binary",
batch_size = 20,
target_size = c(32, 32),
# Randomizing inputs is important so that the NN doesn't get stuck in a rut.
# It's the default, but I've put it in explicitly.
shuffle = TRUE
)
test_image_gen <- image_data_generator()
test_image_flow <- flow_images_from_directory(
directory = "Test/",
generator = test_image_gen,
subset = "test",
class_mode = "binary",
batch_size = 20,
target_size = c(32, 32),
# Randomizing inputs is important so that the NN doesn't get stuck in a rut.
# It's the default, but I've put it in explicitly.
shuffle = TRUE
)
test_image_flow <- flow_images_from_directory(
directory = "Test/",
generator = test_image_gen
test_image_flow <- flow_images_from_directory(
test_image_flow <- flow_images_from_directory(
directory = "Test/",
generator = test_image_gen,
subset = "validation",
class_mode = "binary",
batch_size = 20,
target_size = c(32, 32),
# Randomizing inputs is important so that the NN doesn't get stuck in a rut.
# It's the default, but I've put it in explicitly.
shuffle = TRUE
)
test_validation_image_flow <- flow_images_from_directory(
directory = "Test/",
generator = test_image_gen,
subset = "validation",
class_mode = "binary",
batch_size = 20,
target_size = c(32, 32),
shuffle = FALSE
)
dfmodel <- keras_model_sequential() %>%
# My additions to find dogs and frogs.
layer_dense(units=256, activation="relu") %>%
layer_dropout(rate=0.2) %>%
layer_dense(units=1, activation="sigmoid") %>%
compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy")
dfmodel %>% fit(
x=training_image_flow,
validation_data=valid*ation_image_flow,
epochs=2
)
dfmodel %>% fit(
x = training_image_flow,
validation_data = validation_image_flow,
epochs=2
)
k_clear_session()
dfmodel <- keras_model_sequential() %>%
# My additions to find dogs and frogs.
layer_dense(units=256, activation="relu") %>%
layer_dropout(rate=0.2) %>%
layer_dense(units=1, activation="sigmoid") %>%
compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy")
dfmodel %>% fit(
x = training_image_flow,
validation_data = validation_image_flow,
epochs=2
)
k_clear_session()
dfmodel <- keras_model_sequential() %>%
# My additions to find dogs and frogs.
layer_dense(units=32, activation="relu") %>%
layer_dropout(rate=0.2) %>%
layer_dense(units=1, activation="sigmoid") %>%
compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy")
dfmodel %>% fit(
x = training_image_flow,
validation_data = validation_image_flow,
epochs=2
)
dfmodel <- keras_model_sequential() %>%
layer_dense(units=256, activation="relu") %>%
layer_dropout(rate=0.2) %>%
layer_dense(units=1, activation="sigmoid") %>%
compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy")
dfmodel %>% fit(
x=training_image_flow,
validation_data=validation_image_flow,
epochs=2
)
dfmodel %>% fit(
x = training_image_flow,
validation_data = validation_image_flow,
epochs=1
)
library(tidyverse)
library(keras)
library(knitr)
k_clear_session()
cifar <- dataset_cifar10()
cifar$train$x <- cifar$train$x[cifar$train$y %in% c(5, 6), , , ]/255
cifar$train$y <- cifar$train$y[cifar$train$y %in% c(5, 6)]
cifar$train$y <- factor(if_else(cifar$train$y == 5, "Dog", "Frog"))
cifar$test$x <- cifar$test$x[cifar$test$y %in% c(5, 6), , , ]/255
cifar$test$y <- cifar$test$y[cifar$test$y %in% c(5, 6)]
cifar$test$y <- factor(if_else(cifar$test$y == 5, "Dog", "Frog"))
#dir.create("Train")
#dir.create("Test")
#for(lab in levels(cifar$train$y)){
#  dir.create(paste0("Train/", lab))
#  dir.create(paste0("Test/", lab))
#}
#for(i in 1:dim(cifar$train$x)[1]){
#  png::writePNG(cifar$train$x[i, , , ],
#                paste0("Train/", cifar$train$y[i], "/img", i, ".png"))
#}
#for(i in 1:dim(cifar$test$x)[1]){
#  png::writePNG(cifar$test$x[i, , , ],
#                paste0("Test/", cifar$test$y[i], "/img", i, ".png"))
#}
#rm(cifar)
training_image_gen <- image_data_generator(
rescale = 1/255,
width_shift_range = 0.2,
height_shift_range = 0.2,
rotation_range = 20,
horizontal_flip = TRUE,
validation_split=0.2
)
training_image_flow <- flow_images_from_directory(
directory = "Train/",
generator = training_image_gen,
subset = "training",
class_mode = "binary",
batch_size = 20,
target_size = c(32, 32),
# Randomizing inputs is important so that the NN doesn't get stuck in a rut.
# It's the default, but I've put it in explicitly.
shuffle = TRUE
)
# A separate directory and generator could be used with a fixed validation set.
validation_image_flow <- flow_images_from_directory(
directory = "Train/",
generator = training_image_gen,
subset = "validation",
class_mode = "binary",
batch_size = 20,
target_size = c(32, 32),
shuffle = FALSE
)
dfmodel <- keras_model_sequential() %>%
layer_dense(units=256, activation="relu") %>%
layer_dropout(rate=0.2) %>%
layer_dense(units=1, activation="sigmoid") %>%
compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy")
dfmodel %>% fit(
x = training_image_flow,
validation_data = validation_image_flow,
epochs=1
)
dfmodel %>% fit(
x = training_image_flow
dfmodel %>% fit(
# A separate directory and generator could be used with a fixed validation set.
validation_image_flow <- flow_images_from_directory(
directory = "Test/",
generator = training_image_gen,
subset = "validation",
class_mode = "binary",
batch_size = 20,
target_size = c(32, 32),
shuffle = FALSE
)
dfmodel %>% fit(
x = training_image_flow,
validation_data = validation_image_flow,
epochs=1
)
dfnn <- keras_model_sequential() %>%
# Note that layer_dense expects a "flat" vector, not an image, so we add
# layer_flatten first.
layer_flatten() %>%
layer_dense(units=256, activation="relu", input_shape=c(224, 224, 3)) %>%
layer_dropout(rate=0.2) %>%
layer_dense(units=1, activation="sigmoid") %>%
compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy")
dfnn %>% fit(
x=training_image_flow,
validation_data=validation_image_flow,
epochs=15
)
library(tidyverse)
library(keras)
library(knitr)
k_clear_session()
cifar <- dataset_cifar10()
cifar$train$x <- cifar$train$x[cifar$train$y %in% c(5, 6), , , ]/255
cifar$train$y <- cifar$train$y[cifar$train$y %in% c(5, 6)]
cifar$train$y <- factor(if_else(cifar$train$y == 5, "Dog", "Frog"))
cifar$test$x <- cifar$test$x[cifar$test$y %in% c(5, 6), , , ]/255
cifar$test$y <- cifar$test$y[cifar$test$y %in% c(5, 6)]
cifar$test$y <- factor(if_else(cifar$test$y == 5, "Dog", "Frog"))
#dir.create("Train")
#dir.create("Test")
#for(lab in levels(cifar$train$y)){
#  dir.create(paste0("Train/", lab))
#  dir.create(paste0("Test/", lab))
#}
#for(i in 1:dim(cifar$train$x)[1]){
#  png::writePNG(cifar$train$x[i, , , ],
#                paste0("Train/", cifar$train$y[i], "/img", i, ".png"))
#}
#for(i in 1:dim(cifar$test$x)[1]){
#  png::writePNG(cifar$test$x[i, , , ],
#                paste0("Test/", cifar$test$y[i], "/img", i, ".png"))
#}
#rm(cifar)
training_image_gen <- image_data_generator(
rescale = 1/255,
width_shift_range = 0.2,
height_shift_range = 0.2,
rotation_range = 20,
horizontal_flip = TRUE,
validation_split=0.2
)
training_image_flow <- flow_images_from_directory(
directory = "Train/",
generator = training_image_gen,
subset = "training",
class_mode = "binary",
batch_size = 20,
target_size = c(32, 32),
# Randomizing inputs is important so that the NN doesn't get stuck in a rut.
# It's the default, but I've put it in explicitly.
shuffle = TRUE
)
# A separate directory and generator could be used with a fixed validation set.
validation_image_flow <- flow_images_from_directory(
directory = "Test/",
generator = training_image_gen,
subset = "validation",
class_mode = "binary",
batch_size = 20,
target_size = c(32, 32),
shuffle = FALSE
)
k_clear_session()
dfnn <- keras_model_sequential() %>%
# Note that layer_dense expects a "flat" vector, not an image, so we add
# layer_flatten first.
layer_flatten() %>%
layer_dense(units=256, activation="relu", input_shape=c(32, 32, 3)) %>%
layer_dropout(rate=0.2) %>%
layer_dense(units=1, activation="sigmoid") %>%
compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy")
dfnn %>% fit(
x=training_image_flow,
validation_data=validation_image_flow,
epochs=5
)
eval(dfnn)
dfnn$evaluate
dfnn$evaluate
dfnn %>% fit(
x=training_image_flow,
validation_data=validation_image_flow,
epochs=30
)
history <- dfnn %>% fit(
x=training_image_flow,
validation_data=validation_image_flow,
epochs=10
)
plot(history)
evaluate(dfnn, training_image_flow, validation_image_flow)
dfnn2 <- keras_model_sequential() %>%
layer_dense(units=256, activation="relu") %>%
layer_dropout(rate=0.2) %>%
layer_dense(units=1, activation="sigmoid") %>%
compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy")
history2 <- dfnn2 %>% fit(
x = training_image_flow,
validation_data=validation_image_flow,
epochs=10
)
dfnn2 <- keras_model_sequential() %>%
layer_dense(units=256, activation="relu", input_shape=c(32, 32, 3)) %>%
layer_dropout(rate=0.2) %>%
layer_dense(units=1, activation="sigmoid") %>%
compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy")
history2 <- dfnn2 %>% fit(
x = training_image_flow,
validation_data=validation_image_flow,
epochs=10
)
dfnn2
dfnn2 <- keras_model_sequential() %>%
layer_dense(units=256, activation="relu", input_shape=c(32, 32, 3)) %>%
layer_dropout(rate=0.2) %>%
layer_dense(units=1, activation="sigmoid") %>%
compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy")
history <- dfnn %>% fit(
x=training_image_flow,
validation_data=validation_image_flow,
epochs=10
)
dfnn2 <- keras_model_sequential() %>%
layer_dense(units=256, activation="relu", input_shape=c(32, 32, 3)) %>%
layer_dropout(rate=0.2) %>%
layer_dense(units=1, activation="sigmoid") %>%
compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy")
history2 <- dfnn2 %>% fit(
x=training_image_flow,
validation_data=validation_image_flow,
epochs=10
)
k_clear_session()
dfnn <- keras_model_sequential() %>%
# Note that layer_dense expects a "flat" vector, not an image, so we add
# layer_flatten first.
layer_flatten() %>%
layer_dense(units=256, activation="relu", input_shape=c(32, 32, 3)) %>%
layer_dropout(rate=0.2) %>%
layer_dense(units=1, activation="sigmoid") %>%
compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy")
history <- dfnn %>% fit(
x=training_image_flow,
validation_data=validation_image_flow,
epochs=30
)
plot(history)
evaluate(dfnn, training_image_flow, validation_image_flow)
k_clear_session()
dfnn2 <- keras_model_sequential() %>%
layer_dense(units=256, activation="relu", input_shape=c(32, 32, 3)) %>%
layer_dropout(rate=0.2) %>%
layer_dense(units=1, activation="sigmoid") %>%
compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy")
history2 <- dfnn2 %>% fit(
x=training_image_flow,
validation_data=validation_image_flow,
epochs=10
)
k_clear_session()
dfnn2 <- keras_model_sequential() %>%
layer_dense(units=256, activation="relu", input_shape=c(32, 32, 3)) %>%
layer_dropout(rate=0.2) %>%
layer_dense(units=1, activation="sigmoid") %>%
compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy")
k_clear_session()
fit.conv <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
input_shape = c(64,64,3)) %>%
#layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(rate = 0.25) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%
#layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters = 128, kernel_size = c(3,3), activation = 'relu') %>%
#layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%
layer_dropout(rate = 0.25) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
#layer_dropout(rate = 0.25) %>%
# Because we have a binary predictor, we only need one output unit and need
# the `sigmoid` activation function. That's important!
layer_dense(units = 1, activation = 'sigmoid') %>%
compile(
loss = "binary_crossentropy",
# Here's a run-down of optimizers:
# https://towardsdatascience.com/7-tips-to-choose-the-best-optimizer-47bb9c1219e
optimizer = "adam",
metrics = "accuracy"
)
history2 <- fit.conv %>% fit(
x = training_image_flow,
epochs = 30,
steps_per_epoch = training_image_flow$n/training_image_flow$batch_size,
validation_data = validation_image_flow,
validation_steps = validation_image_flow$n/validation_image_flow$batch_size
)
k_clear_session()
training_image_gen <- image_data_generator(
width_shift_range = 0.2,
height_shift_range = 0.2,
# I really want to try to train on the serif/sans details, so the next two
# ensure that letters are in a variety of orientations.
rotation_range = 45,
horizontal_flip = TRUE,
validation_split=0.2
)
training_image_flow <- flow_images_from_directory(
directory = "Train/",
generator = training_image_gen,
subset = "training",
class_mode = "binary",
batch_size = 20,
target_size = c(32, 32),
# Randomizing inputs is important so that the NN doesn't get stuck in a rut.
# It's the default, but I've put it in explicitly.
shuffle = TRUE
)
# A separate directory and generator could be used with a fixed validation set.
validation_image_flow <- flow_images_from_directory(
directory = "Train/",
generator = training_image_gen,
subset = "validation",
class_mode = "binary",
batch_size = 20,
target_size = c(32, 32),
shuffle = FALSE
)
fit.conv <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
input_shape = c(32,32,3)) %>%
#layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(rate = 0.25) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%
#layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters = 128, kernel_size = c(3,3), activation = 'relu') %>%
#layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%
layer_dropout(rate = 0.25) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
#layer_dropout(rate = 0.25) %>%
# Because we have a binary predictor, we only need one output unit and need
# the `sigmoid` activation function. That's important!
layer_dense(units = 1, activation = 'sigmoid') %>%
compile(
loss = "binary_crossentropy",
# Here's a run-down of optimizers:
# https://towardsdatascience.com/7-tips-to-choose-the-best-optimizer-47bb9c1219e
optimizer = "adam",
metrics = "accuracy"
)
history2 <- fit.conv %>% fit(
x = training_image_flow,
epochs = 30,
steps_per_epoch = training_image_flow$n/training_image_flow$batch_size,
validation_data = validation_image_flow,
validation_steps = validation_image_flow$n/validation_image_flow$batch_size
)
plot(history2)
evaluate(fit.conv, training_image_flow, validation_image_flow)
Title <- c("Accuracy", "Epochs", "Seconds", "Sec/Epoch")
Non-Con <- c(74.20, 30, 257, round(257/30))
Convolutional <- c(86.12, 30, 363, round(363/30))
Non-Con <- c(74.20, 30, 257, round(257/30))
Non_Con <- c(74.20, 30, 257, round(257/30))
Convolutional <- c(86.12, 30, 363, round(363/30))
results <- data.frame(Title, RegularNN, Convolutional)
kable(results)
results <- data.frame(Title, Non_Con, Convolutional)
kable(results)
